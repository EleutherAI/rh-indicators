# GP Exploit Rate Modeling: Approaches, Mathematics, and Results

**Bottom line:** No GP variant outperforms the simple KL divergence slope baseline,
which achieves perfect AUC from just 2 checkpoints. The GP approaches are theoretically
motivated but add complexity without improving prediction.

All GP code is in `src/rh_indicators/trajectory/gp_model.py`.
The evaluation harness is `scripts/binary_emergence_predictor.py`.

---

## 1. Problem Setup

We observe binary outcomes (exploit / no exploit) at each (checkpoint, prefill_level)
cell. At early checkpoints with prefill=0, we typically see 0 exploits out of N trials.
The GP aims to produce better estimates of the true exploit rate at prefill=0 by
borrowing strength from nearby cells and from the IS relationship between prefill levels.

**Notation:**
- $P$: the model checkpoint being evaluated (generates sequences autoregressively)
- $Q$: a fine-tuned model that generates prefill tokens (trained to produce exploits)
- $p_{1:N}$: a length-N prefill sequence
- $\text{rate}_N = E_{p \sim Q}[P(\text{exploit} | p_{1:N})]$: observed exploit rate at prefill level N
- $P(\text{exploit})$: true unprefilled exploit rate (what we want to estimate)

**Evaluation task:** Binary classification of whether each exploit type will exceed
10% exploit rate at prefill=0 at any point during training. N=16 examples (8 exploit
types x 2 training runs). Metric: AUC at each "cutoff index" (number of early
checkpoints visible to the predictor).

---

## 2. GP Models

### 2.1 ExploitRateGP — Unconstrained 2D GP

**Model:**
```
g ~ GP(0, K_Matern52(variance, l_ckpt, l_pfx))
f = g + mu(ckpt)
y_i ~ Binomial(n_i, sigmoid(f_i))
```

The GP places a zero-mean prior on the *deviation* `g` from the mean function. The
full latent function is `f = g + mu(ckpt)`, so `GP(0, K)` means the deviations are
a priori centered at zero — all prior belief about the overall exploit rate level and
trend is captured by the mean function `mu`. The kernel `K` controls how much and how
smoothly `g` can deviate from `mu`.

The GP models log-odds of exploit rate as a smooth function of (log checkpoint,
prefill tokens). Predictions at prefill=0 extrapolate from higher-prefill observations
via the kernel's smoothness.

**Learnable parameters (optimized by L-BFGS-B on Laplace marginal likelihood):**

| Parameter | Description | Typical value |
|-----------|-------------|---------------|
| `variance` | Kernel output scale $\sigma^2_k$. Controls the magnitude of deviations from the mean function. Larger → GP can deviate more from `mu`. | ~1-10 |
| `l_ckpt` | Lengthscale along the log-checkpoint axis. Larger → smoother variation across training. | ~1-5 |
| `l_pfx` | Lengthscale along the prefill-tokens axis. Larger → smoother variation across prefill levels (harder to extrapolate to pfx=0 if too small). | ~10-50 |
| `mu_0` or `b0` | Mean function intercept (log-odds). Sets the prior expected exploit rate. | ~-3 to -5 |
| `b1` | Mean function slope (linear/tanh only). Captures overall trend in exploit rate over training. | ~0-2 |
| `gamma` | Mean function curvature (tanh only). Controls S-curve steepness. 0 → linear, large → saturating. | ~0-5 |

Total learnable parameters: **4** (constant mean), **5** (linear), **6** (tanh).

**Mean function variants:**

| Variant  | Formula                                             | Parameters     |
|----------|-----------------------------------------------------|----------------|
| Constant | mu(t) = mu_0                                        | 1 (mu_0)       |
| Linear   | mu(t) = b0 + b1 * log(ckpt)                        | 2 (b0, b1)     |
| Tanh     | mu(t) = b0 + b1 * tanh(gamma * t_norm) / tanh(gamma) | 3 (b0, b1, gamma) |

Where `t_norm = (log(ckpt) - center) / scale` maps the training range to [-1, 1].
`center` and `scale` are derived from data (not learned). When gamma=0 the tanh
reduces to linear; large gamma gives S-curve saturation at b0 +/- b1. The tanh mean
function was motivated by the expectation that exploit rates follow an S-curve during
training — initially near zero, then rising and saturating.

**Inference:** Laplace approximation (R&W Algorithm 3.1). The binomial Hessian is
diagonal, so the Newton step uses the efficient Cholesky form:
```
W = diag(pi * (1 - pi) * n)    # binomial precision
B = I + W^{1/2} K W^{1/2}
L = chol(B)
g_{new} = K (I - W^{1/2} L^{-T} L^{-1} W^{1/2} K)^{-1} grad_likelihood
```

**Hyperparameter optimization:** L-BFGS-B on the negative Laplace marginal likelihood.
All hyperparameters (kernel variance, lengthscales, mean function params) are optimized
jointly. The latent function values `g` (one per data point) are *not* hyperparameters
— they are found by the Laplace inner loop for each hyperparameter setting.

**Why constant mean works best:** The constant mean has fewest parameters and lets
the kernel do all the work. The linear and tanh means try to capture training dynamics
in the mean function, but with only 8-9 checkpoints the extra parameters overfit or
interfere with the kernel's flexibility. The linear mean in particular can extrapolate
wildly when given few checkpoints.

**Code:** `ExploitRateGP` class, `_laplace_mode()` function.

### 2.2 ExploitRateOffsetGP — 1D GP with KL Offsets

**Model:**
```
g(ckpt) ~ GP(mu, K_Matern52(variance, l_ckpt))     # 1D GP over checkpoints only
f_i = g(ckpt_i) + alpha * KL_i                       # known offsets per observation
y_i ~ Binomial(n_i, sigmoid(f_i))
```

Here `GP(mu, K)` has a *non-zero* mean: `mu` is a learned constant, so the GP
prior on `g(ckpt)` is centered at `mu` rather than 0. Unlike the 2D GP where
`g` is zero-mean and `mu(ckpt)` is added separately, here the mean is baked into
the GP prior itself.

Each (checkpoint, prefill) observation contributes to estimating the same g(checkpoint).
The KL offset captures the IS relationship: `logit(observed rate) ~ logit(true rate)
+ alpha*KL`. At prediction time with offset=0, sigmoid(g(ckpt)) gives the true
unprefilled rate.

**Learnable parameters:**

| Parameter | Description | Typical value |
|-----------|-------------|---------------|
| `variance` | Kernel output scale (1D). | ~1-10 |
| `l_ckpt` | Lengthscale along log-checkpoint. | ~1-5 |
| `mu` | GP mean (constant). Prior expected log-odds of exploit rate at pfx=0. | ~-3 to -5 |
| `alpha` | KL scaling factor. How much each unit of KL shifts log-odds. | ~0.1-0.3 |

Total: **4** learnable parameters. Note only 1 lengthscale (1D GP) and no `l_pfx` —
the prefill dimension is handled entirely by the `alpha * KL` offset.

**Motivation:** Reduce the 2D problem to 1D by absorbing the prefill dimension into
a known offset. This borrows strength more aggressively than the 2D GP — all prefill
levels inform the same latent function.

**Code:** `ExploitRateOffsetGP` class.

### 2.3 ExploitRateConstrainedGP — 2D GP with Pairwise IS Constraints

**Model:**
```
g ~ GP(0, K_Matern52(variance, l_ckpt, l_pfx))  # 2D GP prior on deviations
f = g + mu(ckpt)                                  # latent = deviation + mean function
y_i ~ Binomial(n_i, sigmoid(f_i))                # binomial observations

# Pairwise constraints:
g(ckpt, 0) - g(ckpt, pfx) ~ N(-alpha * KL, sigma_c^2)   for each (ckpt, pfx>0) pair
```

where `sigma_c^2 = sigma_IS^2 + (KL * sigma_slope)^2` (larger slack for higher KL).

Same `GP(0, K)` structure as Section 2.1: zero-mean prior on deviations, with the
mean function `mu(ckpt)` added separately. The key addition is the pairwise
constraints linking pfx=0 and pfx>0 cells at each checkpoint.

The constraints encode the IS relationship: the logit-difference between prefill=0
and prefill=N should be approximately proportional to the KL divergence between P and
Q at that prefill length. This lets high-prefill observations (where exploits are
visible) pull down the pfx=0 estimates in a principled way.

**Learnable parameters:**

*Stage 1 (from unconstrained GP, then fixed):*

| Parameter | Description | Learned in |
|-----------|-------------|------------|
| `variance` | Kernel output scale | Stage 1 |
| `l_ckpt` | Lengthscale, log-checkpoint axis | Stage 1 |
| `l_pfx` | Lengthscale, prefill-tokens axis | Stage 1 |
| `mu_0` / `b0`, `b1`, `gamma` | Mean function params (same as Section 2.1) | Stage 1 |

*Stage 2 (constraint parameters, optionally optimized):*

| Parameter | Description | Default | Learned? |
|-----------|-------------|---------|----------|
| `alpha` | KL scaling factor. Multiplies KL to get constraint offset in logit space. | 0.15 | Optional (2D grid search) |
| `sigma_IS` | Base noise std for constraints. How uncertain we are about the IS relationship. | 1.5 | Optional (2D grid search) |
| `sigma_slope` | Per-unit-KL noise increase. Gives longer prefills (larger KL) more slack. | 0.0 | Fixed |
| `kl_cap` | Hard cap on KL values fed to constraints. | None | Fixed |

Total: **4-6** kernel/mean params (fixed from Stage 1) + **2-4** constraint params
(mostly fixed at defaults). In the default configuration, only the Stage 1 params
are learned; constraint params use hand-tuned defaults.

**Inference:** Direct Newton iteration (not R&W Algorithm 3.1) because the pairwise
constraints couple latent values, making the Hessian non-diagonal:

```
grad = grad_binomial + grad_constraints + grad_prior
H = diag(W) + P + K^{-1}
step = H^{-1} @ grad
```

where P is the sparse constraint precision matrix (off-diagonal entries from the
pairwise links) and W is the binomial precision (diagonal).

**Two-stage fitting** (preferred over joint optimization):
1. Stage 1: Fit unconstrained ExploitRateGP to learn kernel hyperparameters and
   mean function from the data alone.
2. Stage 2: Fix kernel params, find posterior mode under both binomial likelihood
   AND pairwise KL constraints. Optionally optimize alpha and sigma_IS (cheap 2D
   search over the constrained marginal likelihood).

Joint optimization of kernel params + constraint params tends to fail because the
optimizer weakens the kernel to accommodate constraints (e.g., inflating l_pfx to
ignore the prefill dimension entirely, or reducing kernel variance so constraints
dominate).

**Code:** `ExploitRateConstrainedGP` class, `_laplace_mode_constrained()` function.

### 2.4 Jensen Gap Variance Correction (Experimental)

**Modification to Section 2.3:** Replace the heuristic `alpha * KL` constraint
offset with a data-driven correction based on the second-order Jensen gap estimate.

For each (checkpoint, prefill) cell c:
```
V_c = Var_{samples in c}(kl_divergence)    # variance of per-sample KL
jensen_gap_c = V_c / 2                      # second-order approximation
offset_c = max(KL_c - jensen_gap_c, 0)      # corrected constraint offset
sigma_c = sqrt(sigma_IS^2 + V_c)            # per-cell uncertainty
```

**Motivation:** The IS theory (Section 3) shows the Jensen bound gets looser with
prefill length. The gap is approximately `Var(log w)/2` where `log w = log P(p) -
log Q(p)` are the log importance weights. Since `kl_divergence = -log w`, we can
estimate the gap directly from the per-sample KL variance within each cell.

**Why it failed:** The per-sample KL variance is much larger than the mean KL in
nearly all cells:
```
Median KL = 10.7, Median Var(kl) = 31.7
→ Jensen correction = Var/2 = 15.8 > KL = 10.7
→ Corrected offset = max(10.7 - 15.8, 0) = 0
```

This effectively deletes all constraints (68 of 72 cells get offset=0), reducing the
constrained GP to an unconstrained GP. The high variance is not a bug — it reflects
genuine variability in per-sample KL (each sample involves different prefill tokens,
so log P(p)/Q(p) varies substantially). Between-exploit-type variance contributes
minimally; the variance is dominated by within-type, within-cell variability.

The second-order Jensen gap approximation (Var/2) is unreliable when the coefficient
of variation of the per-sample KL is 0.3-1.0, as it is here. Higher-order terms in
the gap expansion are significant.

Additionally, we compute `Var(log w) = Var(kl)` but the actual Jensen gap requires
`Var(log w + log r)/2` including the exploit outcome term. Since `log w` and `log r`
are negatively correlated (Q selects prefills where P(exploit|p) is high), the true
combined variance could be substantially smaller than `Var(log w)` alone.

**Code:** `jensen_correction` parameter in `fit_two_stage()` and
`compute_constrained_gp_features_at_cutoff()`. CLI flag `--jensen-correction`.

---

## 3. Importance Sampling Theory

### 3.1 The IS Identity

Prefilled evaluation is importance sampling. P generates sequences; Q provides prefills.
By the law of total probability:

```
P(exploit) = E_{p ~ P}[P(exploit | p_{1:N})]     (exact, all possible prefills)
```

The observed rate uses Q-sampled prefills:

```
rate_N = E_{p ~ Q}[P(exploit | p_{1:N})]
```

Connecting via IS:

```
P(exploit) = E_{p ~ Q}[(P(p)/Q(p)) * P(exploit | p)]     (exact IS identity)
```

Since Q is fine-tuned to produce exploit-eliciting prefills, P(p)/Q(p) is small where
P(exploit|p) is high. The importance weights are negatively correlated with exploit
probability, so:

```
P(exploit) = rate_N + Cov_{p~Q}(P(p)/Q(p), P(exploit|p))
                       ^ negative
```

Therefore **rate_N overestimates the true rate**. The prefill-assisted exploit rate is
an upper bound on the true unprefilled rate (in expectation).

### 3.2 The Jensen Bound (Log Space)

The IS identity is exact but involves weights P(p)/Q(p) that can be astronomically
small (e^{-50} for a 50-token prefill). Standard IS is numerically infeasible —
the importance weights are effectively zero in floating point.

Working in log space and applying Jensen's inequality (log is concave):

```
log P(exploit) = log E_{p~Q}[(P(p)/Q(p)) * P(exploit|p)]
               >= E_{p~Q}[log P(exploit|p) + log P(p) - log Q(p)]     [Jensen]
               = E_{p~Q}[log P(exploit|p)] - KL(Q || P)
```

where `KL(Q || P) = E_{p~Q}[log Q(p) - log P(p)]` is the KL divergence of the
prefill distributions.

This gives a **lower bound** on the log exploit rate: the observed average log exploit
probability minus the KL divergence between the prefill source and the evaluation
model. This is the theoretical motivation for the constrained GP's use of KL as the
constraint offset.

### 3.3 The Jensen Gap and Prefill Length

The Jensen gap is:

```
Gap = log E_{p~Q}[w*r] - E_{p~Q}[log(w*r)]
```

where w = P(p)/Q(p) and r = P(exploit|p). Since log E[w*r] = log P(exploit) is
**fixed** (independent of prefill length N):

```
Gap = log P(exploit) + KL(Q||P) - E_{p~Q}[log P(exploit|p)]
```

As N (prefill length) increases:
- **KL(Q_N || P_N) grows**: each additional token adds divergence between Q and P.
  The KL is a sum over N token positions, so it grows roughly linearly with N.
- **E[log P(exploit|p)] plateaus**: once the prefill has switched the model into
  exploit mode (~10-20 tokens), extra tokens don't increase the exploit rate much.
- **Gap grows**: driven entirely by KL accumulation without corresponding rate
  increase. The bound gets progressively looser.

**Mechanistically:** By the second-order expansion of Jensen's inequality, the gap
is approximately `Var_{p~Q}(log w + log r) / 2`. The log importance weights
`log P(p)/Q(p)` are sums over N tokens, so their variance grows with N (more terms
in the sum → random-walk-like variance accumulation). Meanwhile `log P(exploit|p)`
stabilizes. Late prefill tokens add noise to the importance weights without adding
signal about exploitation.

**This explains why alpha << 1:** The constrained GP uses `alpha * KL` as the
constraint offset. If the Jensen bound were tight, alpha would be 1.0. But the gap
grows with KL, so the effective mapping from KL to logit-difference is sublinear —
captured by alpha ≈ 0.15.

### 3.4 Estimating the Jensen Gap

The gap is estimable from data. For each sample i with prefill p_i:

- `log w_i = log P(p_i) - log Q(p_i)` is computable from logprobs under both models.
  We already have `kl_divergence = log Q(p) - log P(p) = -log w` per sample.
- `log r_i = log P(exploit | p_i)` is observable (binary outcome per sample).

The empirical variance of `log w_i + log r_i` across samples gives an estimate of the
Jensen gap via the second-order approximation.

**Practical limitation:** We attempted to use `Var(kl_divergence) = Var(log w)` as a
proxy (Section 2.4). This fails because (a) the variance is so large that the
second-order approximation breaks down (CoV ≈ 0.3-1.0), and (b) it omits the
`log r` term which is negatively correlated with `log w`.

### 3.5 Connection to the Constrained GP

The constrained GP uses the IS relationship as:

```
f(ckpt, 0) - f(ckpt, pfx) ~ N(-alpha * KL, sigma_c^2)
```

This is a linearization of the Jensen bound:
- **Constraint mean (-alpha * KL):** The logit-difference between pfx=0 and pfx=N
  should scale with KL, but sublinearly (alpha << 1) because the Jensen gap grows.
- **Constraint variance (sigma_c^2):** Captures uncertainty in the relationship.
  The heuristic `sigma_IS^2 + (KL * sigma_slope)^2` gives longer prefills more
  slack.
- **kl_cap:** Prevents pathologically large constraints at very high KL.

The heuristic parameters (alpha, sigma_slope, kl_cap) are crude but effective
approximations of the theoretically motivated per-cell Jensen gap correction. The
principled correction (Section 2.4) turns out to be too aggressive — it effectively
removes all constraints rather than merely weakening them appropriately.

---

## 4. Binary Emergence Prediction Results

### 4.1 Task Description

Predict whether each exploit type will exceed 10% exploit rate at prefill=0 (unprompted
exploitation) at any point during training. Evaluation uses 8 exploit types x 2 training
runs = 16 labeled examples. For each "cutoff index" k, the predictor sees only the first
k+1 checkpoints and must classify all 16 examples. AUC is computed for each cutoff.

**Metrics evaluated:**
- `mean_neg_kl`: negative mean KL divergence across (problem, prefill) pairs, averaged
  over checkpoints seen. Direct measure of how different P is from Q.
- `exploit_rate`: observed exploit rate averaged across prefill levels and checkpoints.
- `log_exploit_lower_bound`: log of `max_prefill[P(prefill) * P(exploit|prefill)]`,
  a scaling-law-based lower bound on unprefilled exploit rate.
- `gp_log_rate_p0`: GP-estimated log exploit rate at prefill=0, using all checkpoints
  seen (fitted surface).
- `gp_log_rate_p0_last`: GP-estimated log exploit rate at the last checkpoint only.

**Models evaluated:**
- `threshold_level`: AUC of the metric itself as a classifier (no parameters).
- `threshold_slope`: AUC of the metric's slope (change between consecutive cutoffs).
- `logistic_level_slope`: Logistic regression on level + slope with LOO-CV.

### 4.2 Baseline Results (No GP)

These baselines use only the raw metrics without any GP modeling:

| Metric                  | Model           | idx 0 | idx 1 | idx 2 | idx 3 | idx 4 | idx 5 | idx 6 | idx 7 |
|-------------------------|-----------------|-------|-------|-------|-------|-------|-------|-------|-------|
| mean_neg_kl             | threshold_slope | 0.500 | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** |
| mean_neg_kl             | logistic        | 0.000 | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** |
| mean_neg_kl             | threshold_level | 0.547 | 0.594 | 0.828 | 0.891 | **1.000** | **1.000** | **1.000** | **1.000** |
| log_exploit_lower_bound | threshold_slope | 0.500 | 0.906 | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** |
| exploit_rate            | threshold_level | 0.469 | 0.406 | 0.703 | **1.000** | **1.000** | **1.000** | **1.000** | **1.000** |

**Key finding:** The slope of mean KL divergence achieves perfect AUC from just 2
checkpoints (idx 1). This is the strongest baseline. The *level* of KL is much
weaker — it doesn't reach perfect AUC until idx 4 (5 checkpoints). The slope
captures whether the model is becoming *more different* from Q over training, which
is the key signal.

### 4.3 Unconstrained GP Results (ExploitRateGP)

Three mean function variants were tested. All use the constrained GP (Section 2.3)
with two-stage fitting and default hyperparameters (alpha=0.15, sigma_IS=1.5).

#### gp_log_rate_p0 (fitted surface, all checkpoints)

| Cutoff | Constant | Linear | Tanh  | Best baseline |
|--------|----------|--------|-------|---------------|
| 0      | 0.578    | 0.578  | 0.578 | 0.547         |
| 1      | 0.500    | 0.477  | 0.555 | **1.000**     |
| 2      | 0.812    | 0.734  | 0.734 | **1.000**     |
| 3      | **1.000**| 0.969  | 0.984 | **1.000**     |
| 4      | **1.000**| **1.000** | 0.875 | **1.000** |
| 5      | 0.875    | **1.000** | 0.891 | **1.000** |
| 6      | **1.000**| 0.891  | 0.875 | **1.000**     |
| 7      | **1.000**| 0.891  | 0.766 | **1.000**     |

#### gp_log_rate_p0_last (last checkpoint only)

| Cutoff | Constant | Linear | Tanh  | Best baseline |
|--------|----------|--------|-------|---------------|
| 0      | 0.445    | 0.445  | 0.445 | 0.547         |
| 1      | 0.484    | 0.203  | 0.328 | **1.000**     |
| 2      | 0.625    | 0.703  | 0.703 | **1.000**     |
| 3      | 0.969    | **1.000** | **1.000** | **1.000** |
| 4      | 0.906    | 0.875  | 0.891 | **1.000**     |
| 5      | 0.875    | **1.000** | 0.766 | **1.000** |
| 6      | **1.000**| 0.750  | **1.000** | **1.000** |
| 7      | **1.000**| 0.875  | 0.859 | **1.000**     |

**Analysis:**

- **Constant mean** is the most stable GP variant. It reaches AUC=1.0 at idx 3 for
  `gp_log_rate_p0` (threshold_level) and maintains it at most cutoffs.
- **Linear mean** is volatile — excellent at some cutoffs (1.000) but drops badly at
  others (0.203 at idx 1, 0.750 at idx 6). The linear extrapolation of the mean
  function is unstable with few checkpoints.
- **Tanh mean** performs similarly to linear on average but with different failure
  modes. The extra gamma parameter doesn't consistently help.
- **All GP variants first reach AUC=1.0 at idx 3** (4 checkpoints), compared to
  the KL slope baseline which reaches AUC=1.0 at idx 1 (2 checkpoints). The GP
  needs more data to produce reliable extrapolations.

### 4.4 Jensen Gap Variance Correction Results

The Jensen correction (Section 2.4) was tested with the constant mean GP:

#### gp_log_rate_p0 (fitted surface)

| Cutoff | Constant (no correction) | Jensen correction | Best baseline |
|--------|--------------------------|-------------------|---------------|
| 0      | 0.578                    | 0.578             | 0.547         |
| 1      | 0.500                    | 0.438             | **1.000**     |
| 2      | 0.812                    | 0.641             | **1.000**     |
| 3      | **1.000**                | 0.750             | **1.000**     |
| 4      | **1.000**                | 0.969             | **1.000**     |
| 5      | 0.875                    | 0.812             | **1.000**     |
| 6      | **1.000**                | 0.664             | **1.000**     |
| 7      | **1.000**                | 0.672             | **1.000**     |

#### gp_log_rate_p0_last (last checkpoint)

| Cutoff | Constant (no correction) | Jensen correction | Best baseline |
|--------|--------------------------|-------------------|---------------|
| 0      | 0.445                    | 0.445             | 0.547         |
| 1      | 0.484                    | 0.312             | **1.000**     |
| 2      | 0.625                    | 0.688             | **1.000**     |
| 3      | 0.969                    | 0.875             | **1.000**     |
| 4      | 0.906                    | **1.000**         | **1.000**     |
| 5      | 0.875                    | 0.766             | **1.000**     |
| 6      | **1.000**                | 0.766             | **1.000**     |
| 7      | **1.000**                | 0.688             | **1.000**     |

**The Jensen correction makes results uniformly worse.** It never reaches stable
AUC=1.0 — achieving it once at idx 4 for `gp_log_rate_p0_last` but regressing
to 0.688 by idx 7. The correction effectively disables all constraints because
`Var(kl)/2 > KL` for 95% of cells (see Section 2.4), turning the constrained GP
into an unconstrained GP that cannot borrow strength from high-prefill observations.

### 4.5 Summary Table

First cutoff index achieving AUC = 1.0 (and whether it's sustained):

| Method                        | First perfect | Sustained? |
|-------------------------------|---------------|------------|
| mean_neg_kl slope             | **idx 1**     | Yes        |
| mean_neg_kl logistic          | **idx 1**     | Yes        |
| log_exploit_lower_bound slope | idx 2         | Yes        |
| exploit_rate level            | idx 3         | Yes        |
| GP constant, gp_log_rate_p0   | idx 3         | Mostly (dips at idx 5) |
| GP linear, gp_log_rate_p0_last| idx 3         | No (volatile) |
| GP tanh, gp_log_rate_p0_last  | idx 3         | No (volatile) |
| GP Jensen, gp_log_rate_p0     | Never         | —          |

---

## 5. Why the GP Approaches Don't Help

Detecting a *trend* is simpler than estimating a *probability*, and attempting the
latter introduces noise into the former. The KL slope baseline asks a simple question
— is the model becoming more similar to the exploit-trained model over training? —
and answers it robustly from just 2 checkpoints. The GP instead tries to estimate
the actual exploit rate at pfx=0 by modeling the full 2D rate surface and
extrapolating, which requires fitting kernel parameters, mean functions, and
constraint weights, all of which introduce estimation error that contaminates the
final ranking.

---

## 6. Code Reference

| Component | Location |
|-----------|----------|
| Matern-5/2 kernel | `gp_model.py:matern52_kernel()` |
| Laplace approximation (unconstrained) | `gp_model.py:_laplace_mode()` |
| Laplace approximation (constrained) | `gp_model.py:_laplace_mode_constrained()` |
| ExploitRateGP | `gp_model.py:ExploitRateGP` |
| ExploitRateOffsetGP | `gp_model.py:ExploitRateOffsetGP` |
| ExploitRateConstrainedGP | `gp_model.py:ExploitRateConstrainedGP` |
| Data preparation (constrained) | `gp_model.py:prepare_constrained_gp_data()` |
| Data preparation (offset) | `gp_model.py:prepare_offset_gp_data()` |
| GP feature computation | `gp_model.py:compute_gp_features()` |
| Constrained GP features | `gp_model.py:compute_constrained_gp_features()` |
| Per-cutoff GP features | `gp_model.py:compute_constrained_gp_features_at_cutoff()` |
| Binary emergence predictor | `scripts/binary_emergence_predictor.py` |
| Prefill logprob computation | `scripts/compute_prefill_logprobs.py` |
| Exploit rate scaling (Laplace) | `src/rh_indicators/trajectory/scaling.py` |
