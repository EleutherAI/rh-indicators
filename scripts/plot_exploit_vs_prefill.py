#!/usr/bin/env python3
"""
Plot exploit rate vs prefill tokens for each checkpoint.

This visualization shows how prefill sensitivity changes across training:
- X-axis: Number of prefill tokens
- Y-axis: Exploit rate (fraction of problems exploited)
- One line per checkpoint

Usage:
    python scripts/plot_exploit_vs_prefill.py \
        --run-dir results/prefill_sensitivity/prefill_sensitivity-20251216-012007-47bf405 \
        --output plots/exploit_vs_prefill.png
"""

import argparse
import json
import re
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def parse_filename(filename: str) -> tuple[int, int] | None:
    """Parse checkpoint and prefill values from filename.

    Expected format: checkpoint-{N}_prefill{M}.jsonl
    Returns (checkpoint_num, prefill_tokens) or None if pattern doesn't match.
    """
    match = re.match(r"checkpoint-(\d+)_prefill(\d+)\.jsonl$", filename)
    if match:
        return int(match.group(1)), int(match.group(2))
    return None


def load_per_problem_results(input_dir: Path) -> pd.DataFrame:
    """Load per-problem results from all checkpoint Ã— prefill eval files."""
    rows = []

    for jsonl_file in sorted(input_dir.glob("checkpoint-*_prefill*.jsonl")):
        if ".samples." in jsonl_file.name:
            continue

        parsed = parse_filename(jsonl_file.name)
        if parsed is None:
            continue

        checkpoint, prefill_tokens = parsed

        with open(jsonl_file, "r") as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    rows.append({
                        "task_id": record.get("task_id"),
                        "checkpoint": checkpoint,
                        "prefill_tokens": prefill_tokens,
                        "exploit_success": record.get("exploit_success", False),
                        "exploit_type": record.get("exploit_type"),
                    })

    return pd.DataFrame(rows)


def compute_exploit_rates(df: pd.DataFrame) -> pd.DataFrame:
    """Compute exploit rate for each (checkpoint, prefill_tokens) combination.

    Uses "any exploit" aggregation: a problem counts as exploited if any attempt succeeds.
    """
    # First aggregate within task_id to get any-exploit
    task_level = df.groupby(
        ["checkpoint", "prefill_tokens", "task_id"]
    )["exploit_success"].max().reset_index()

    # Then compute mean across tasks
    rates = task_level.groupby(
        ["checkpoint", "prefill_tokens"]
    )["exploit_success"].agg(["mean", "count", "sum"]).reset_index()

    rates.columns = ["checkpoint", "prefill_tokens", "exploit_rate", "n_problems", "n_exploited"]

    return rates


def plot_exploit_vs_prefill(
    rates: pd.DataFrame,
    output_path: Path,
    title: str = "Exploit Rate vs Prefill Tokens by Checkpoint",
) -> None:
    """Plot exploit rate vs prefill for each checkpoint."""
    fig, ax = plt.subplots(figsize=(10, 6))

    checkpoints = sorted(rates["checkpoint"].unique())
    colors = plt.cm.viridis(np.linspace(0, 1, len(checkpoints)))

    for ckpt, color in zip(checkpoints, colors):
        ckpt_data = rates[rates["checkpoint"] == ckpt].sort_values("prefill_tokens")
        ax.plot(
            ckpt_data["prefill_tokens"],
            ckpt_data["exploit_rate"],
            marker='o',
            label=f"Checkpoint {ckpt}",
            color=color,
            linewidth=2,
            markersize=6,
        )

    ax.set_xlabel("Prefill Tokens", fontsize=12)
    ax.set_ylabel("Exploit Rate", fontsize=12)
    ax.set_title(title, fontsize=14)
    ax.legend(loc="best", title="Checkpoint")
    ax.grid(True, alpha=0.3)
    ax.set_ylim(0, None)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"Plot saved to: {output_path}")

    pdf_path = output_path.with_suffix('.pdf')
    plt.savefig(pdf_path, bbox_inches='tight')
    print(f"PDF saved to: {pdf_path}")

    plt.close()


def plot_exploit_vs_prefill_by_type(
    df: pd.DataFrame,
    output_path: Path,
    title: str = "Exploit Rate vs Prefill by Exploit Type",
) -> None:
    """Plot exploit rate vs prefill, one subplot per exploit type."""
    exploit_types = sorted(df["exploit_type"].dropna().unique())
    n_types = len(exploit_types)

    if n_types == 0:
        print("No exploit types found, skipping by-type plot")
        return

    n_cols = min(3, n_types)
    n_rows = (n_types + n_cols - 1) // n_cols

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
    if n_types == 1:
        axes = [[axes]]
    elif n_rows == 1:
        axes = [axes]

    checkpoints = sorted(df["checkpoint"].unique())
    colors = plt.cm.viridis(np.linspace(0, 1, len(checkpoints)))

    for idx, exploit_type in enumerate(exploit_types):
        row, col = divmod(idx, n_cols)
        ax = axes[row][col] if n_rows > 1 else axes[0][col]

        type_df = df[df["exploit_type"] == exploit_type]

        # Compute rates for this type
        task_level = type_df.groupby(
            ["checkpoint", "prefill_tokens", "task_id"]
        )["exploit_success"].max().reset_index()

        rates = task_level.groupby(
            ["checkpoint", "prefill_tokens"]
        )["exploit_success"].mean().reset_index()
        rates.columns = ["checkpoint", "prefill_tokens", "exploit_rate"]

        for ckpt, color in zip(checkpoints, colors):
            ckpt_data = rates[rates["checkpoint"] == ckpt].sort_values("prefill_tokens")
            if len(ckpt_data) > 0:
                ax.plot(
                    ckpt_data["prefill_tokens"],
                    ckpt_data["exploit_rate"],
                    marker='o',
                    label=f"Ckpt {ckpt}",
                    color=color,
                    linewidth=1.5,
                    markersize=4,
                )

        ax.set_xlabel("Prefill Tokens")
        ax.set_ylabel("Exploit Rate")
        ax.set_title(exploit_type.replace("_", " ").title())
        ax.grid(True, alpha=0.3)
        ax.set_ylim(0, 1)

    # Hide empty subplots
    for idx in range(n_types, n_rows * n_cols):
        row, col = divmod(idx, n_cols)
        ax = axes[row][col] if n_rows > 1 else axes[0][col]
        ax.set_visible(False)

    # Add legend to first subplot
    if n_types > 0:
        axes[0][0].legend(loc="upper left", fontsize=8)

    fig.suptitle(title, fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"By-type plot saved to: {output_path}")

    pdf_path = output_path.with_suffix('.pdf')
    plt.savefig(pdf_path, bbox_inches='tight')

    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="Plot exploit rate vs prefill tokens",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--run-dir",
        type=Path,
        required=True,
        help="Run directory containing evals/",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Output path for main plot (default: run-dir/plots/exploit_vs_prefill.png)",
    )
    parser.add_argument(
        "--by-type",
        action="store_true",
        help="Also generate per-exploit-type plots",
    )
    parser.add_argument(
        "--exclude-tasks",
        type=str,
        nargs="+",
        default=[],
        help="Task IDs to exclude from analysis (e.g., false positives)",
    )

    args = parser.parse_args()

    evals_dir = args.run_dir / "evals"
    if not evals_dir.exists():
        print(f"Error: {evals_dir} does not exist")
        return 1

    # Default output path
    if args.output is None:
        plots_dir = args.run_dir / "plots"
        plots_dir.mkdir(parents=True, exist_ok=True)
        args.output = plots_dir / "exploit_vs_prefill.png"
    else:
        args.output.parent.mkdir(parents=True, exist_ok=True)

    # Load data
    print(f"Loading data from {evals_dir}...")
    df = load_per_problem_results(evals_dir)
    print(f"Loaded {len(df)} rows")

    # Filter out excluded tasks
    if args.exclude_tasks:
        before = len(df)
        df = df[~df["task_id"].isin(args.exclude_tasks)]
        excluded = before - len(df)
        print(f"Excluded {excluded} rows from {len(args.exclude_tasks)} task(s): {args.exclude_tasks}")

    checkpoints = sorted(df["checkpoint"].unique())
    prefills = sorted(df["prefill_tokens"].unique())
    print(f"Checkpoints: {checkpoints}")
    print(f"Prefill tokens: {prefills}")

    # Compute rates
    rates = compute_exploit_rates(df)
    print("\nExploit rates:")
    print(rates.pivot(index="prefill_tokens", columns="checkpoint", values="exploit_rate"))

    # Save rates CSV
    rates_path = args.output.parent / "exploit_rates.csv"
    rates.to_csv(rates_path, index=False)
    print(f"\nRates saved to: {rates_path}")

    # Plot
    plot_exploit_vs_prefill(rates, args.output)

    if args.by_type:
        by_type_path = args.output.with_stem(args.output.stem + "_by_type")
        plot_exploit_vs_prefill_by_type(df, by_type_path)

    return 0


if __name__ == "__main__":
    exit(main())
